{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.23.0\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes==0.41.1\n",
      "  Using cached bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: datasets==2.13.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (2.13.0)\n",
      "Requirement already satisfied: openai==0.28.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (0.28.1)\n",
      "Requirement already satisfied: peft==0.4.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (0.4.0)\n",
      "Collecting safetensors==0.4.0\n",
      "  Using cached safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting transformers==4.34.0\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: trl==0.4.7 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (0.4.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from accelerate==0.23.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from accelerate==0.23.0) (23.2)\n",
      "Requirement already satisfied: psutil in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from accelerate==0.23.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from accelerate==0.23.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from accelerate==0.23.0) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from accelerate==0.23.0) (0.22.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (14.0.2)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from fsspec[http]>=2021.11.1->datasets==2.13.0) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from datasets==2.13.0) (3.9.1)\n",
      "Requirement already satisfied: filelock in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from transformers==4.34.0) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from transformers==4.34.0) (2023.12.25)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0)\n",
      "  Using cached tokenizers-0.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from aiohttp->datasets==2.13.0) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from aiohttp->datasets==2.13.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from aiohttp->datasets==2.13.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from aiohttp->datasets==2.13.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from aiohttp->datasets==2.13.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from aiohttp->datasets==2.13.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.23.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.13.0) (2023.11.17)\n",
      "Collecting huggingface-hub (from accelerate==0.23.0)\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.23.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.23.0) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from pandas->datasets==2.13.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from pandas->datasets==2.13.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from pandas->datasets==2.13.0) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate==0.23.0) (1.3.0)\n",
      "Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "Using cached bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "Using cached safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Using cached tokenizers-0.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Installing collected packages: bitsandbytes, safetensors, huggingface-hub, tokenizers, transformers, accelerate\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.43.0\n",
      "    Uninstalling bitsandbytes-0.43.0:\n",
      "      Successfully uninstalled bitsandbytes-0.43.0\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Uninstalling safetensors-0.4.2:\n",
      "      Successfully uninstalled safetensors-0.4.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.1\n",
      "    Uninstalling huggingface-hub-0.22.1:\n",
      "      Successfully uninstalled huggingface-hub-0.22.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.39.2\n",
      "    Uninstalling transformers-4.39.2:\n",
      "      Successfully uninstalled transformers-4.39.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.28.0\n",
      "    Uninstalling accelerate-0.28.0:\n",
      "      Successfully uninstalled accelerate-0.28.0\n",
      "Successfully installed accelerate-0.23.0 bitsandbytes-0.41.1 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \\\n",
    "accelerate==0.23.0 \\\n",
    "bitsandbytes==0.41.1 \\\n",
    "datasets==2.13.0 \\\n",
    "openai==0.28.1 \\\n",
    "peft==0.4.0 \\\n",
    "safetensors==0.4.0 \\\n",
    "transformers==4.34.0 \\\n",
    "trl==0.4.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py7zr in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: texttable in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in /home/bio/anaconda3/envs/amne/lib/python3.9/site-packages (from py7zr) (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bio/anaconda3/envs/amne/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Dataset({\n",
      "    features: ['CHQ', 'Summary'],\n",
      "    num_rows: 800\n",
      "})\n",
      "\n",
      "Test Dataset:\n",
      "Dataset({\n",
      "    features: ['CHQ', 'Summary'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"MeQSum.csv\")\n",
    "\n",
    "# Convert the DataFrame to a datasets.Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training and test datasets\n",
    "train_dataset = dataset.select(range(800))\n",
    "test_dataset = dataset.select(range(800, len(dataset)))\n",
    "\n",
    "# Print information about the datasets\n",
    "print(\"Train Dataset:\")\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['CHQ', 'Summary'],\n",
      "    num_rows: 800\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHQ': 'SUBJECT: who and where to get cetirizine - D\\nMESSAGE: I need/want to know who manufscturs Cetirizine. My Walmart is looking for a new supply and are not getting the recent',\n",
       " 'Summary': 'Who manufactures cetirizine?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bio/anaconda3/envs/amne/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "access_token = \"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\"  # Replace with your actual token\n",
    "\n",
    "# Set up the configuration for quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Authenticate and load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=access_token  # Use the token for authentication\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "You are a helpful, respectful and honest assistant. Your task is to summarize the following consumer health query. Your answer should be based on the provided consumer health query only.\n",
      "\n",
      "### Consumer Health Query:\n",
      "SUBJECT: who and where to get cetirizine - D\n",
      "MESSAGE: I need/want to know who manufscturs Cetirizine. My Walmart is looking for a new supply and are not getting the recent\n",
      "\n",
      "### Summary:\n",
      "Who manufactures cetirizine? </s>\n"
     ]
    }
   ],
   "source": [
    "def prompt_formatter(sample):\n",
    "\treturn f\"\"\"<s>### Instruction:\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Your task is to summarize the following consumer health query. \\\n",
    "Your answer should be based on the provided consumer health query only.\n",
    "\n",
    "### Consumer Health Query:\n",
    "{sample['CHQ']}\n",
    "\n",
    "### Summary:\n",
    "{sample['Summary']} </s>\"\"\"\n",
    "\n",
    "n = 0\n",
    "print(prompt_formatter(train_dataset[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/bio/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "#\n",
    "# construct a Peft model.\n",
    "# the QLoRA paper recommends LoRA dropout = 0.05 for small models (7B, 13B)\n",
    "#\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "#\n",
    "# set up the trainer\n",
    "#\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama2-7b-chat-meqsum\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=4,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,  # Disable bf16 precision\n",
    "    fp16=False,   # Enable fp16 precision\n",
    "    tf32=True,  # Disable tf32 precision (optional, depends on your setup)\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_formatter,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/bio/anaconda3/envs/amne/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 4/200 [01:09<56:03, 17.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0949, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [02:17<54:15, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9086, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [03:24<52:59, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8385, 'learning_rate': 0.0002, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [04:32<51:51, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.683, 'learning_rate': 0.0002, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [04:49<51:36, 16.92s/it]/home/bio/anaconda3/envs/amne/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 10%|█         | 20/200 [05:36<49:05, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6814, 'learning_rate': 0.0002, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 24/200 [06:43<49:03, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5921, 'learning_rate': 0.0002, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [07:51<48:18, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5701, 'learning_rate': 0.0002, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 32/200 [08:58<47:18, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5774, 'learning_rate': 0.0002, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 35/200 [09:45<45:58, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 585.1593, 'train_samples_per_second': 2.734, 'train_steps_per_second': 0.342, 'train_loss': 1.7266004834856306, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=1.7266004834856306, metrics={'train_runtime': 585.1593, 'train_samples_per_second': 2.734, 'train_steps_per_second': 0.342, 'train_loss': 1.7266004834856306, 'epoch': 1.18})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run inference using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_folder = \"llama2-7b-chat-meqsum\"\n",
    "\n",
    "# load both the adapter and the base model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are a helpful, respectful and honest assistant. Your task is to summarize the following consumer  health query. Your answer should be based on the provided text only.\n",
      "\n",
      "### Consumer Health Query:\n",
      "SUBJECT: Questions (see in comment box) on \"Vistaril\"\n",
      "MESSAGE: I have a few Qs related to \"Vistaril\":\n",
      "1. Is \"Vistaril\" a gluten-free product?\n",
      "2. If not, is it because some of the raw materials in its ingredients that is gluten related?\n",
      "3. If yes to Q1, is it a \"Certified\" gluten-free product and what kind of certification it has?\n",
      "4. For capsule form, does it contain any kind of starch? If it does, what starch is it?\n",
      "5.  Can you email to me a list of the ingredients in this drug?\n",
      "Thank you very much!\n",
      "\n",
      "### Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = test_dataset[1]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Your task is to summarize the following consumer  health query. \\\n",
    "Your answer should be based on the provided text only.\n",
    "\n",
    "### Consumer Health Query:\n",
    "{sample['CHQ']}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " What is the ingredients in Vistaril and is it gluten-free? \n",
      "\n",
      "Ground truth:\n",
      " What are the ingredients of vistaril and is it gluten and starch free?\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=50, temperature=0.7)\n",
    "\n",
    "print('Output:\\n',\n",
    "      tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):])\n",
    "print('\\nGround truth:\\n', sample['Summary'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
